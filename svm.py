# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YvzObkRucGeLKJA1Wj9u13vJqoBdHn0H
"""

import pandas as pd
import os
from skimage.transform import resize
from skimage.io import imread
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

df = pd.read_csv('/content/drive/MyDrive/Code Projects/Data/IAP UROP DATA/gray_data.csv')
img_path =  "/content/drive/MyDrive/Code Projects/Data/IAP UROP DATA/larvae_images" #path to images labeled correctly for model evaluation
images = img_path

df.head(10)

#data statisitcs
print('number of dead shellfish -------')
print(len(df.loc[df['Target']==1]))
print('number of alive shelfish--------')
print(len(df.loc[df['Target']==2]))

def equalize_data(df):
  alives = df.loc[df['Target']==2]
  deads = df.loc[df['Target']==1]
  size_alives = len(alives)
  size_deads = len(deads)

  if size_alives > size_deads:
    sample_df = alives.sample(n=size_deads, random_state=1)
  else:
    sample_df = alives.sample(n=size_alives, random_state=1)

  df = pd.concat([sample_df, deads], ignore_index=True)

  #data statisitcs
  print('number of dead shellfish -------')
  print(len(df.loc[df['Target']==1]))
  print('number of alive shelfish--------')
  print(len(df.loc[df['Target']==2]))

  return df

df = equalize_data(df)

print('splitting data')
#input data
x=df.iloc[:,1:-1]
#output data
y=df.iloc[:,-1]

# Splitting the data into training and testing sets
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.20, random_state=77, stratify=y)
print(x_train.head())

#data statisitcs of train, test, split
print(y_train.value_counts())
print(y_test.value_counts())

print('definig starting parameters')
# Defining the parameters grid for GridSearchCV
param_grid = {
    'C':[0.01,0.1],
    'kernel' : ["rbf",'poly'],
    'gamma' : [0.001,0.01]
}

# Creating a support vector classifier
print('creating classfier')
svc=svm.SVC(probability=True,verbose=True)

# Creating a model using GridSearchCV with the parameters grid
print('creating model')
model=GridSearchCV(svc,param_grid,cv=5,verbose=True)

# Training the model using the training data
print('fitting model.....')
clf = model.fit(x_train,y_train)

print('testing model.....')
# Testing the model using the testing data
y_pred = model.predict(x_test)

print("Best Parameters:",model.best_params_)

print("Train Score:",model.best_score_)

print("Test Score:",model.score(x_test,y_test))

print('getting accuracy....')
print(' ')
# Calculating the accuracy of the model
accuracy = accuracy_score(y_pred, y_test)

# Print the accuracy of the model
print(f"The model is {accuracy*100}% accurate")

#getting performance statisitcs
print(classification_report(y_test, y_pred, target_names=['1', '2']))

from sklearn.metrics import confusion_matrix as cm
cm = cm(y_test, y_pred)
print(cm)
#print(x_test.head())

#save the model
import pickle
pickle.dump(model,open('/content/square_model_','wb')) #saving in google colab

#How to load the model:
#with open('model.pickle','rb') as f:
#   model_loaded = pcikle.load(open(file_path,'rb'))

import joblib

# save model with joblib
filename = 'svm_joblib_model.sav'
joblib.dump(model, filename)

cnt = 0

for row_index, (prediction, label) in enumerate(zip (y_pred, y_test)):
  if prediction != label:
    print('Row', row_index, 'has been classified as ', prediction, 'and should be ', label)

numbers = []
for row_index, (prediction, label) in enumerate(zip (y_pred, y_test)):
  if prediction != label:
    numbers.append(row_index)
    print('Row', row_index, 'has been classified as ', prediction, 'and should be ', label)

  data = []

for filename in os.listdir(images):
    if filename.endswith("jpg"):
        # Your code comes here such as
        print(filename)
        data.append(filename)